<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="x-ua-compatible" content="IE=9" />
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <link rel="stylesheet" type="text/css" href="../../include/manstyle.css" />
  <link rel="icon" type="image/png" href="../../images/ccgicon.png" />
  <link rel="shortcut icon" type="image/png" href="../../images/ccgicon.png" />
  <meta name="keywords" content="keywords:" />
  <meta name="keywords" content="functions: hclust_tree" />
  <meta name="keywords" content="functions: hclust_id" />
  <meta name="keywords" content="functions: hclust_list" />
  <meta name="keywords" content="functions: hclust_info" />
  <title>Clustering Functions</title>
</head>
<body>
<div class="center-page">
  <!-- START MOE_HEADER -->
  <div class="MOE_HEADER">
    <a href="../../index.htm"><img src="../../images/ccglogo.png" /></a>
    <a href="../../index.htm"><div class="moeversion"></div></a>
    Clustering Functions
  </div>
<!-- END MOE_HEADER -->
<div class="content">

<h1>Syntax</h1>

<pre class="code">
htree        = <a class="fcnlink" href=
"#hclust_tree">hclust_tree</a> ['method', dist_data, opt]

cluster_list = <a class="fcnlink" href=
"#hclust_list">hclust_list</a> [htree, maxscore]
cluster_id   = <a class="fcnlink" href=
"#hclust_id">hclust_id</a> [htree, maxscore]

info         = <a class="fcnlink" href=
"#hclust_info">hclust_info</a> ['operation', htree, opt]
</pre>

<h1>Description</h1>

<p>These functions provide a low-level interface for clustering arbitrary
  data. Data to be clustered can be represented in one of three ways: as a
  square distance matrix, with element <i>(i, j)</i> giving the distance
  between items <i>i</i> and <i>j</i>; as a symmetric neighbor list and
  corresponding set of distances; or as a laminated vector of triplets <i>(i,
  j, d)</i> where <i>d</i> gives the distance between items <i>i</i> and
  <i>j</i>. If the data provided in any of the three formats is non-symmetric,
  the minimum distance given for each pair will be used. Self-distances are
  always zero, regardless of what the input data state.</p>

<p>Several agglomerative hierarchical clustering methods are provided. All
  such methods result in a dendrogram. For a set V = [v<sub>1</sub>,
  v<sub>2</sub>, .. v<sub>N</sub>], a dendrogram is a set of subsets
  c<sub>k</sub>, and corresponding distances d<sub>k</sub>, such that:</p>
  <ol>
    <li>Any two sets are either distinct or one is a proper subset of the
    other</li>
    <li>For any two c<sub>i</sub>, c<sub>j</sub>, if c<sub>i</sub> is a subset
    of c<sub>j</sub> then d<sub>i</sub>&lt;d<sub>j</sub></li>
    <li>The sets cover V, i.e. the union of all c<sub>k</sub>s is
    [v<sub>1</sub>, v<sub>2</sub>, .. v<sub>N</sub>]</li>
  </ol>

<p>A dendrogram tree consists of tree nodes and leaf nodes. Leaf nodes
  represent the clustered objects. Each tree node has exactly two children,
  which can be either tree or leaf nodes. Each tree node is labelled with a
  score which is the distance or 'dissimilarity' between its two child
  nodes. When the children are both leaves, this distance is the distance
  between the two objects they represent, in the input data. Otherwise this
  distance is computed in a way that is dependent on the clustering method
  employed but is a function of the child leaf nodes.</p>
<a id="hclust_tree"></a>
<hr noshade="noshade" />

<pre>htree = <span class=
  "fcndef">hclust_tree</span> ['method', dist_data, opt]
</pre>

<p>This is the main clustering function, returning a dendrogram of the
  clustered items. When the data is sparse - that is, the distance between many
  pairs of items is large (effectively infinite) - <tt>dist_data</tt> should be
  provided in the neighbor list or edge list format. When the distances between
  all items are already known or most distances are expected to be below the
  desired cutoff, the full matrix can be provided. The Lance-Williams
  [Lance&nbsp;1967] matrix-update formulae are used to merge two columns into a
  single column/row and update the matrix accordingly based on the clustering
  method.</p>

<p>Four different hierarchical agglomerative methods are supported:</p>
  <ul>
    <li><b>single-linkage</b>. Any two items within the distance cutoff are
    placed within the same cluster, i.e. no two items from two distinct
    clusters are within the cutoff distance. Tends to produce long noodly
    clusters. This is also called nearest neighbor clustering.</li>
    <li><b>complete-linkage</b>. All pairs of items within a given cluster are
    within the cutoff distance from each other. Tends to produce tight, compact
    clusters.</li>
    <li><b>average-linkage</b>. Generally results in clusters larger than those
    of complete-linkage clustering but smaller than those of single-linkage
    clustering.</li>
    <li><b>ward</b> [Ward&nbsp;1963]. Similar to average-linkage but tries to
    produce homogeneous clusters, minimizing variation inside the
    clusters.</li>
  </ul>

<p><tt>method</tt> must be one of: <tt>'single'</tt>,
  <tt>'complete'</tt>, <tt>'average'</tt> or
  <tt>'ward'</tt>.</p>

<p><tt>dist_data</tt> should be one of:</p>
  <ul>
    <li>A nested NxN matrix of pairwise distances between the N items being
    clustered. For non-symmetric entries, the smaller distance will be
    used.</li>
    <li>A vector v of length two. The first element should be a vector of
    length N, with the <i>i</i>th element listing the indices of all the
    neighbors of item <i>i</i> (i.e. those with less than infinite distance).
    The second element of v should be a vector of distances having the same
    shape as the neighbor list, giving the distance between each corresponding
    pair. The distance from each item to itself is assumed to be zero and need
    not be given explicitly. For non-symmetric or repeated entries, the
    smallest distance will be used.</li>
    <li>A vector v of length three. The first two elements together form a list
    of pairs or edges, specified as indices of corresponding items that are
    'close'; the third element gives the corresponding distances
    between each pair. The distance from each item to itself is assumed to be
    zero and need not be given explicitly. For non-symmetric or repeated
    entries, the smallest distance will be used.</li>
  </ul>

<p>The options given in <tt>opt</tt> are as follows:</p>
<dl>
<dt><tt>nclusters : n</tt></dt>
<dd>Stops the agglomeration procedure once the number of remaining clusters
is n. This can be used to exit early. The default value is 1 (i.e. run to
completion).</dd>
<dt><tt>maxsize : s</tt></dt>
<dd>Stops the agglomeration procedure once the number of elements in the
largest cluster reaches s. This can be used to exit early. The default
value is N, the number of elements (i.e. run to completion).</dd>
<dt><tt>maxscore : cutoff_value</tt></dt>
<dd>Determines the dissimilarity cutoff for clustering. Any items which are
within this distance are considered to be neighbors, while those further
apart are not. Larger cutoffs will result in fewer, larger clusters and
generally longer computation time. The default is infinity.</dd>
<dt><tt>progress : prog_fcn</tt></dt>
<dd>
  If given, the specified function will be called once each iteration, with
  a tagged vector argument consisting of:
  <ul>
    <li><b>curstep</b> - the current step number</li>
    <li><b>totstep</b> - the total number of steps assuming running to
    completion of the full dendrogram</li>
    <li><b>curscore</b> - score of the most recently joined nodes</li>
  </ul>

<pre class="code">local function prog_fcn prog
    write ['Step {n:}/{n:}\n', prog.curstep, prog.totstep];
endfunction
</pre>
  </dd>
  <dt><tt>verbose : flag</tt></dt>
  <dd>If non-zero, progress messages will be printed out to the SVL Commands
  window.</dd>
  <dt><tt>htree : htree</tt></dt>
  <dd>If given, resumes clustering from a previously built tree, which must
  be the return value from an earlier call to this function. This allows one
  to change the clustering method partway through, for example.</dd>
</dl>

<a id="hclust_list"></a>
<a id="hclust_id"></a>
<hr noshade="noshade" />

<pre>cluster_list = <span class="fcndef">hclust_list</span> [htree, maxscore]
cluster_id = <span class="fcndef">hclust_id</span> [htree, maxscore]
</pre>

<p><tt>hclust_list</tt> and <tt>hclust_id</tt> return the clustered items.
  These take as parameters a dendrogram, <tt>htree</tt>, as returned by
  <tt>hclust_tree</tt>, and a <tt>maxscore</tt> distance which will determine
  the number of clusters actually produced. Only items nearer to each other
  than this cutoff distance will be considered to be neighbors. Note that if
  <tt>maxscore</tt> or one of the other early stopping conditions was used when
  building the tree, the tree will already be cut accordingly. Thus, specifying
  a value equal to or greater than the previous cutoff will have no further
  effect, although specifying a smaller cutoff may result in additional
  clusters being created in this case.</p>

<p>The only difference between the two functions is that <tt>hclust_list</tt>
  returns a vector that is a list of indices split up by cluster, so that
  <tt>clusters(i)</tt> gives the indices of the members of the <i>i</i>th
  cluster. Each index from 1 up to N appears in exactly one cluster. On the
  other hand, <tt>hclust_id</tt> returns a flat vector of length N giving the
  cluster number to which the <i>i</i>th element belongs. The cluster numbers
  are arbitrary but start at 1 and increase consecutively.</p>
<a id="hclust_info"></a>
<hr noshade="noshade" />

<pre>info = <span class=
  "fcndef">hclust_info</span> ['operation', htree, opt]
</pre>

<p>Allows for the extraction of various types of information from the
  <tt>htree</tt> data structure used by the clustering functions.
  <tt>htree</tt> should be the return value from a call to
  <tt>hclust_tree</tt>, while <tt>operation</tt> is a token that specifies the
  type of information desired. <tt>opt</tt> may be used to pass in additional
  information required for some of the operations. The valid operations
  are:</p>
<dl>
  <dt><tt>score</tt></dt>
  <dd>Returns a vector giving, for each non-leaf node of the tree, the
  distance between its immediate children. If the clustering was terminated
  early, only the distances for the nodes clustered up to the stopping point
  will be returned.</dd>
  <dt><tt>size</tt></dt>
  <dd>Returns a vector giving, for each non-leaf node of the tree, the number
  of leaf nodes below it. If the clustering was terminated early, only the
  counts for the nodes clustered up to the stopping point will be
  returned.</dd>
  <dt><tt>nleafs</tt></dt>
  <dd>Returns a scalar giving the total number of objects clustered.</dd>
</dl>

<h1>See Also</h1>

<p><a href="../../svl/fcnref/graphlib.htm">Graph Functions</a></p>

<h1>References</h1>
<table class="ref">
  <tbody>
    <tr>
      <td>[Ward&nbsp;1963]</td>
      <td>Ward, J.H.; Hierarchical Grouping to Optimize an
      Objective Function; <i>J. Amer. Statist. Assoc. 58</i> (<b>1963</b>)
      236-244.</td>
    </tr>
    <tr>
      <td>[Lance&nbsp;1967]</td>
      <td>Lance, G.N., Williams, W.T.; A General Theory of
      Classificatory Sorting Strategies. 1. Hierarchical Systems; <i>Computer
      J. 9</i> (<b>1967</b>) 373.</td>
    </tr>
  </tbody>
</table><!-- START MOE_FOOTER -->
  <div class="MOE_FOOTER">
    <img src="../../images/ccgicon.png" /> <a href="../../index.htm"></a>
    <a href="../../legal.htm"></a> &copy;<span class="versionyear"></span>
    <a href="http://www.chemcomp.com"></a>. All rights reserved.<br />
    <a href="mailto:info@chemcomp.com"></a>
  </div><!-- END MOE_FOOTER -->
</div>
</div>
</body>
</html>

